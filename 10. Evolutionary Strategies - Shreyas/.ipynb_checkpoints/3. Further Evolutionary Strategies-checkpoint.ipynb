{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "num_params = len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(x):\n",
    "    assert x.ndim == 1\n",
    "    ranks = np.empty(len(x), dtype=int)\n",
    "    ranks[x.argsort()] = np.arange(len(x))\n",
    "    return ranks\n",
    "\n",
    "def norm_ranks(x):\n",
    "    ranks = get_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "    ranks /= (x.size - 1)\n",
    "    ranks -= 0.5\n",
    "    return ranks\n",
    "\n",
    "def get_w_decay(w_decay, params_list):\n",
    "    params_array = np.array(params_list)\n",
    "    return -w_decay * np.mean(params_array*params_array, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, pi, epsilon=1e-08):\n",
    "        self.pi = pi\n",
    "        self.dim = pi.num_params\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, globalg):\n",
    "        self.t += 1\n",
    "        step = self._compute_step(globalg)\n",
    "        theta = self.pi.mu\n",
    "        ratio = np.linalg.norm(step) / (np.linalg.norm(theta) + self.epsilon)\n",
    "        self.pi.mu = theta + step\n",
    "        return ratio\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BasicSGD(Optimizer):\n",
    "    def __init__(self, pi, stepsize):\n",
    "        Optimizer.__init__(self, pi)\n",
    "        self.stepsize = stepsize\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        step = -self.stepsize * globalg\n",
    "        return step\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, pi, stepsize, momentum=0.9):\n",
    "        Optimizer.__init__(self, pi)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.stepsize, self.momentum = stepsize, momentum\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        self.v = self.momentum * self.v + (1. - self.momentum) * globalg\n",
    "        step = -self.stepsize * self.v\n",
    "        return step\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, pi, stepsize, beta1=0.99, beta2=0.999):\n",
    "        Optimizer.__init__(self, pi)\n",
    "        self.stepsize = stepsize\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = np.zeros(self.dim, dtype=np.float32)\n",
    "        self.v = np.zeros(self.dim, dtype=np.float32)\n",
    "\n",
    "    def _compute_step(self, globalg):\n",
    "        a = self.stepsize * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * globalg\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (globalg * globalg)\n",
    "        step = -a * self.m / (np.sqrt(self.v) + self.epsilon)\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(W):\n",
    "    X = env.reset()\n",
    "    for t in range(1,201):\n",
    "        action = 0 if W@X < 0 else 1\n",
    "        X, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            return t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenES:\n",
    "    \"\"\"Basic version of OpenAI ES\"\"\"\n",
    "    def __init__(self, \n",
    "                 num_params, \n",
    "                 sig_init=0.1, \n",
    "                 sig_decay=0.999, \n",
    "                 sig_lim=0.01, \n",
    "                 w_decay=1e-2,  \n",
    "                 popsize=256, \n",
    "                 lr=1e-2, \n",
    "                 lr_decay=0.9999, \n",
    "                 lr_lim=1e-3, \n",
    "                 antithetic=False, \n",
    "                 rank_fit=True, \n",
    "                 forget_best=True):\n",
    "        \n",
    "        self.num_params = num_params\n",
    "        self.popsize = popsize\n",
    "        self.sig = sig_init\n",
    "        self.sig_decay = sig_decay\n",
    "        self.sig_lim = sig_lim\n",
    "        self.w_decay = w_decay\n",
    "        self.lr = lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_lim = lr_lim\n",
    "        self.antithetic = antithetic\n",
    "        if self.antithetic:\n",
    "            assert (self.popsize%2 == 0), \"Popsize should be even\"\n",
    "            self.popsize_h = int(self.popsize/2)\n",
    "        \n",
    "        self.forget_best = forget_best\n",
    "        self.rank_fit = rank_fit\n",
    "        if self.rank_fit: self.forget_best = True\n",
    "        \n",
    "        self.mu = np.zeros(self.num_params)\n",
    "        #self.reward = np.zeros(self.popsize)\n",
    "        self.best_mu = self.mu\n",
    "        self.best_reward = 0\n",
    "        self.first_gen = True\n",
    "        self.optimizer = Adam(self, self.lr)\n",
    "        \n",
    "    def ask(self):\n",
    "            if self.antithetic:\n",
    "                self.epsilon_h = np.random.randn(self.popsize_h, self.num_params)\n",
    "                self.epsilon = np.concatenate(self.epsilon_h, -self.epsilon_h)\n",
    "            else: self.epsilon = np.random.randn(self.popsize, self.num_params)\n",
    "            \n",
    "            self.solution = self.mu.reshape(1, self.num_params) + self.epsilon * self.sig\n",
    "    \n",
    "            return self.solution\n",
    "        \n",
    "    def tell(self, reward_list):\n",
    "            assert (len(reward_list)==self.popsize), \"Inconsistent reward size\"\n",
    "            r = np.array(reward_list)\n",
    "            \n",
    "            if self.rank_fit: rewards = norm_ranks(r)\n",
    "            if self.w_decay > 0: \n",
    "                l2_decay = get_w_decay(self.w_decay, self.solution)\n",
    "                rewards += l2_decay\n",
    "                \n",
    "            norm_rewards = (rewards + np.mean(rewards)) / np.std(rewards)\n",
    "            mu_delta = (self.epsilon.T @ norm_rewards) / (self.popsize*self.sig)\n",
    "            \n",
    "            self.optimizer.stepsize = self.lr\n",
    "            update_ratio = self.optimizer.update(-mu_delta)\n",
    "            \n",
    "            if self.sig > self.sig_lim: self.sig *= self.sig_decay\n",
    "            if self.lr > self.lr_lim: self.lr *= self.lr_decay\n",
    "            \n",
    "            idx = rewards.argsort()[::-1]\n",
    "            \n",
    "            self.best_reward = r[idx[0]]\n",
    "            self.best_mu = self.solution[idx[0]]\n",
    "            \n",
    "            if self.first_gen:\n",
    "                self.first_gen = False\n",
    "                self.best_reward_ = self.best_reward\n",
    "                self.best_mu_ = self.best_mu\n",
    "            else:\n",
    "                if self.forget_best or (self.best_reward > self.best_reward_):\n",
    "                    self.best_reward_ = self.best_reward\n",
    "                    self.best_mu_ = self.best_mu\n",
    "                \n",
    "    def result(self):\n",
    "            return self.best_mu, self.best_reward_, self.best_reward, self.sig\n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_REQUIRED_FITNESS = 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 200.0 200.0\n"
     ]
    }
   ],
   "source": [
    "solver = OpenES(num_params)\n",
    "e = 0\n",
    "while e < 100:\n",
    "\n",
    "    # ask the ES to give us a set of candidate solutions\n",
    "    solutions = solver.ask()\n",
    "\n",
    "    # create an array to hold the fitness results.\n",
    "    fitness_list = np.zeros(solver.popsize)\n",
    "\n",
    "    # evaluate the fitness for each given solution.\n",
    "    for i in range(solver.popsize):\n",
    "        fitness_list[i] = evaluate(solutions[i])\n",
    "\n",
    "    # give list of fitness results back to ES\n",
    "    solver.tell(fitness_list)\n",
    "\n",
    "    # get best parameter, fitness from ES\n",
    "    best_solution, best_fitness_ever, best_fitness_current, sigma = solver.result()\n",
    "    e += 1\n",
    "    print(e, best_fitness_ever, best_fitness_current)\n",
    "    #print (best_fitness)\n",
    "    if best_fitness_ever > MY_REQUIRED_FITNESS:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
